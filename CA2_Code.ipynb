{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5aed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69ac448",
   "metadata": {},
   "outputs": [],
   "source": [
    "customSchema = StructType([\n",
    "    StructField(\"Primary_Index\", IntegerType(), True), \n",
    "    StructField(\"Tweet_Id\", StringType(), True), \n",
    "    StructField(\"Date_Text\", StringType(), True),\n",
    "    StructField(\"Flag\", StringType(), True), \n",
    "    StructField(\"User\", StringType(), True),\n",
    "    StructField(\"Tweet_Text\", StringType(), True)])\n",
    "\n",
    "df = spark.read.load('hdfs://localhost:9000/CA2/ProjectTweets.csv', format=\"csv\", header=\"False\", sep=',', schema=customSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb02d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove commas from the Tweet Text field\n",
    "##First test an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0bb486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+\n",
      "|Tweet_Text                                                                                         |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?|\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(df[\"Tweet_Id\"] == '1467811594')\n",
    "# Select only the \"Tweet_Text\" column from the filtered DataFrame\n",
    "result = filtered_df.select(\"Tweet_Text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcd4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Then strip the commas out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec30428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+\n",
      "|Tweet_Text                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit only a bit  LOL  I'm fine thanks  how's you ?|\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "# Remove commas from the \"Tweet_Text\" column\n",
    "\n",
    "filtered_df = df.filter(df[\"Tweet_Id\"] == '1467811594')\n",
    "\n",
    "filtered_df = filtered_df.withColumn(\"Tweet_Text\", regexp_replace(filtered_df[\"Tweet_Text\"], \",\", \"\"))\n",
    "\n",
    "# Select only the \"Tweet_Text\" column from the filtered DataFrame\n",
    "result = filtered_df.select(\"Tweet_Text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2fa28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now do it with all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1888c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Tweet_Text\", regexp_replace(df[\"Tweet_Text\"], \",\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622797b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+\n",
      "|Tweet_Text                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit only a bit  LOL  I'm fine thanks  how's you ?|\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(df[\"Tweet_Id\"] == '1467811594')\n",
    "# Select only the \"Tweet_Text\" column from the filtered DataFrame\n",
    "result = filtered_df.select(\"Tweet_Text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa63413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of Primary_Index: 7\n",
      "Max length of Tweet_Id: 10\n",
      "Max length of Date_Text: 28\n",
      "Max length of Flag: 8\n",
      "Max length of User: 15\n",
      "Max length of Tweet_Text: 374\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the max length of each column\n",
    "from pyspark.sql.functions import max, length\n",
    "max_lengths = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    max_length = df.agg(max(length(column_name))).collect()[0][0]\n",
    "    max_lengths.append((column_name, max_length))\n",
    "\n",
    "# Display the results\n",
    "for col, max_len in max_lengths:\n",
    "    print(f\"Max length of {col}: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4388a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in Primary_Index: 0\n",
      "Number of null values in Tweet_Id: 0\n",
      "Number of null values in Date_Text: 0\n",
      "Number of null values in Flag: 0\n",
      "Number of null values in User: 0\n",
      "Number of null values in Tweet_Text: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    count = df.select(column_name).where(col(column_name).isNull()).count()\n",
    "    null_counts.append((column_name, count))\n",
    "\n",
    "# Display the results\n",
    "for col, count in null_counts:\n",
    "    print(f\"Number of null values in {col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb930ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in Primary_Index: 1600000\n",
      "Number of unique values in Tweet_Id: 1598315\n",
      "Number of unique values in Date_Text: 774363\n",
      "Number of unique values in Flag: 1\n",
      "Number of unique values in User: 659775\n",
      "Number of unique values in Tweet_Text: 1581312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:>                                                         (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Calculate the number of unique values in each column\n",
    "unique_counts = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    count = df.agg(countDistinct(column_name)).collect()[0][0]\n",
    "    unique_counts.append((column_name, count))\n",
    "\n",
    "# Display the results\n",
    "for col, count in unique_counts:\n",
    "    print(f\"Number of unique values in {col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7065ca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 78:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  Tweet_Id|count|\n",
      "+----------+-----+\n",
      "|1469531660|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Find and display an example record with duplicated \"Tweet_Id\"\n",
    "duplicate_tweet_id_example = df.groupBy(\"Tweet_Id\").count().filter(col(\"count\") > 1).limit(1)\n",
    "\n",
    "# Display the result\n",
    "duplicate_tweet_id_example.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bf779b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|Primary_Index|Tweet_Id  |Date_Text                   |Flag    |User        |Tweet_Text                                                       |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|6730         |1469531660|Tue Apr 07 06:39:53 PDT 2009|NO_QUERY|appleaddicto|Company blocked Twitter today  oh well i still have it on mobile |\n",
      "|809639       |1469531660|Tue Apr 07 06:39:53 PDT 2009|NO_QUERY|appleaddicto|Company blocked Twitter today  oh well i still have it on mobile |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find and display all records with the Tweet_Id \"1469531660\"\n",
    "matching_records = df.filter(col(\"Tweet_Id\") == \"1469531660\")\n",
    "\n",
    "# Display the matching records\n",
    "matching_records.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c469069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with unique records\n",
    "unique_records_df = df.dropDuplicates([\"Tweet_Id\", \"Date_Text\", \"Flag\", \"User\", \"Tweet_Text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f72334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 21:10:06,473 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-29 21:10:06,502 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-29 21:10:07,120 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-29 21:10:07,144 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 120:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in Primary_Index: 1598315\n",
      "Number of unique values in Tweet_Id: 1598315\n",
      "Number of unique values in Date_Text: 774363\n",
      "Number of unique values in Flag: 1\n",
      "Number of unique values in User: 659775\n",
      "Number of unique values in Tweet_Text: 1581312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 122:>                                                        (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique values in each column\n",
    "unique_counts = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    count = unique_records_df.agg(countDistinct(column_name)).collect()[0][0]\n",
    "    unique_counts.append((column_name, count))\n",
    "\n",
    "# Display the results\n",
    "for col, count in unique_counts:\n",
    "    print(f\"Number of unique values in {col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41641c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 21:10:51,477 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-29 21:10:51,484 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-29 21:10:51,736 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-29 21:10:51,744 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 130:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|Primary_Index|Tweet_Id  |Date_Text                   |Flag    |User        |Tweet_Text                                                       |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|6730         |1469531660|Tue Apr 07 06:39:53 PDT 2009|NO_QUERY|appleaddicto|Company blocked Twitter today  oh well i still have it on mobile |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Find and display all records with the Tweet_Id \"1469531660\"\n",
    "matching_records = unique_records_df.filter(col(\"Tweet_Id\") == \"1469531660\")\n",
    "\n",
    "# Display the matching records\n",
    "matching_records.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18036c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unique_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b90d8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 157:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------------+--------------------+\n",
      "|  Tweet_Id|           Date_Text|          User|          Tweet_Text|  Tweet_Text_Cleaned|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+\n",
      "|1467878557|Mon Apr 06 22:37:...|    tshamysboo|dierks bentley is...|dierks bentley is...|\n",
      "|1467901424|Mon Apr 06 22:43:...|      duckyrat|This latest episo...|This latest episo...|\n",
      "|1467918595|Mon Apr 06 22:48:...|     DanaSully|@DaveLindquist ca...|@DaveLindquist ca...|\n",
      "|1467982795|Mon Apr 06 23:06:...|      TiffinyB|I am home missing...|I am home missing...|\n",
      "|1468020773|Mon Apr 06 23:17:...| MalaysianLife|@daydreamer20 Goo...|@daydreamer20 Goo...|\n",
      "|1468021367|Mon Apr 06 23:18:...|DanielChambers|Back on the 7am t...|Back on the 7am t...|\n",
      "|1468023239|Mon Apr 06 23:18:...|   mayveleezet|day 2 is almost o...|day 2 is almost o...|\n",
      "|1468036593|Mon Apr 06 23:22:...|      johnhays|@realchrissystar ...|@realchrissystar ...|\n",
      "|1468071517|Mon Apr 06 23:33:...|       JeanneD|Is there Twitter ...|Is there Twitter ...|\n",
      "|1468087930|Mon Apr 06 23:38:...| _DanielTouch_|is eating frozen ...|is eating frozen ...|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = unique_records_df.select(\"Tweet_Id\", \"Date_Text\", \"User\",\"Tweet_Text\", col(\"Tweet_Text\").alias(\"Tweet_Text_Cleaned\"))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fdd6bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 158:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Date_Text_Characters|\n",
      "+--------------------+\n",
      "|                 PDT|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 158:============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract characters at positions 21, 22, and 23 and select unique values\n",
    "from pyspark.sql.functions import substring\n",
    "unique_characters = df.select(substring(\"Date_Text\", 21, 3).alias(\"Date_Text_Characters\")).distinct()\n",
    "\n",
    "# Show the unique characters\n",
    "unique_characters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cd47077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df =df.withColumn(\"Year\", df.Date_Text.substr(-4, 4))\n",
    "df = df.withColumn(\"Month\", df.Date_Text.substr(5, 3))\n",
    "df = df.withColumn(\"Day\", df.Date_Text.substr(9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edbdcfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+\n",
      "|  Tweet_Id|           Date_Text|          User|          Tweet_Text|  Tweet_Text_Cleaned|Year|Month|Day|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+\n",
      "|1467878557|Mon Apr 06 22:37:...|    tshamysboo|dierks bentley is...|dierks bentley is...|2009|  Apr| 06|\n",
      "|1467901424|Mon Apr 06 22:43:...|      duckyrat|This latest episo...|This latest episo...|2009|  Apr| 06|\n",
      "|1467918595|Mon Apr 06 22:48:...|     DanaSully|@DaveLindquist ca...|@DaveLindquist ca...|2009|  Apr| 06|\n",
      "|1467982795|Mon Apr 06 23:06:...|      TiffinyB|I am home missing...|I am home missing...|2009|  Apr| 06|\n",
      "|1468020773|Mon Apr 06 23:17:...| MalaysianLife|@daydreamer20 Goo...|@daydreamer20 Goo...|2009|  Apr| 06|\n",
      "|1468021367|Mon Apr 06 23:18:...|DanielChambers|Back on the 7am t...|Back on the 7am t...|2009|  Apr| 06|\n",
      "|1468023239|Mon Apr 06 23:18:...|   mayveleezet|day 2 is almost o...|day 2 is almost o...|2009|  Apr| 06|\n",
      "|1468036593|Mon Apr 06 23:22:...|      johnhays|@realchrissystar ...|@realchrissystar ...|2009|  Apr| 06|\n",
      "|1468071517|Mon Apr 06 23:33:...|       JeanneD|Is there Twitter ...|Is there Twitter ...|2009|  Apr| 06|\n",
      "|1468087930|Mon Apr 06 23:38:...| _DanielTouch_|is eating frozen ...|is eating frozen ...|2009|  Apr| 06|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84757b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Month|\n",
      "+-----+\n",
      "|  May|\n",
      "|  Jun|\n",
      "|  Apr|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the `distinct` method to get unique values in the \"Month\" column\n",
    "unique_months = df.select(\"Month\").distinct()\n",
    "\n",
    "# Show the unique values\n",
    "unique_months.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a5c975b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+\n",
      "|  Tweet_Id|           Date_Text|          User|          Tweet_Text|  Tweet_Text_Cleaned|Year|Month|Day|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+\n",
      "|1467878557|Mon Apr 06 22:37:...|    tshamysboo|dierks bentley is...|dierks bentley is...|2009|   04| 06|\n",
      "|1467901424|Mon Apr 06 22:43:...|      duckyrat|This latest episo...|This latest episo...|2009|   04| 06|\n",
      "|1467918595|Mon Apr 06 22:48:...|     DanaSully|@DaveLindquist ca...|@DaveLindquist ca...|2009|   04| 06|\n",
      "|1467982795|Mon Apr 06 23:06:...|      TiffinyB|I am home missing...|I am home missing...|2009|   04| 06|\n",
      "|1468020773|Mon Apr 06 23:17:...| MalaysianLife|@daydreamer20 Goo...|@daydreamer20 Goo...|2009|   04| 06|\n",
      "|1468021367|Mon Apr 06 23:18:...|DanielChambers|Back on the 7am t...|Back on the 7am t...|2009|   04| 06|\n",
      "|1468023239|Mon Apr 06 23:18:...|   mayveleezet|day 2 is almost o...|day 2 is almost o...|2009|   04| 06|\n",
      "|1468036593|Mon Apr 06 23:22:...|      johnhays|@realchrissystar ...|@realchrissystar ...|2009|   04| 06|\n",
      "|1468071517|Mon Apr 06 23:33:...|       JeanneD|Is there Twitter ...|Is there Twitter ...|2009|   04| 06|\n",
      "|1468087930|Mon Apr 06 23:38:...| _DanielTouch_|is eating frozen ...|is eating frozen ...|2009|   04| 06|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Use the `when` and `lit` functions to replace values in the \"Month\" column\n",
    "df = df.withColumn(\"Month\", when(df[\"Month\"] == \"May\", lit(\"05\"))\n",
    "                        .when(df[\"Month\"] == \"Apr\", lit(\"04\"))\n",
    "                        .when(df[\"Month\"] == \"Jun\", lit(\"06\"))\n",
    "                        .otherwise(df[\"Month\"]))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7720a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 166:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+----------+\n",
      "|  Tweet_Id|           Date_Text|          User|          Tweet_Text|  Tweet_Text_Cleaned|Year|Month|Day|      Date|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+----------+\n",
      "|1467878557|Mon Apr 06 22:37:...|    tshamysboo|dierks bentley is...|dierks bentley is...|2009|   04| 06|2009-04-06|\n",
      "|1467901424|Mon Apr 06 22:43:...|      duckyrat|This latest episo...|This latest episo...|2009|   04| 06|2009-04-06|\n",
      "|1467918595|Mon Apr 06 22:48:...|     DanaSully|@DaveLindquist ca...|@DaveLindquist ca...|2009|   04| 06|2009-04-06|\n",
      "|1467982795|Mon Apr 06 23:06:...|      TiffinyB|I am home missing...|I am home missing...|2009|   04| 06|2009-04-06|\n",
      "|1468020773|Mon Apr 06 23:17:...| MalaysianLife|@daydreamer20 Goo...|@daydreamer20 Goo...|2009|   04| 06|2009-04-06|\n",
      "|1468021367|Mon Apr 06 23:18:...|DanielChambers|Back on the 7am t...|Back on the 7am t...|2009|   04| 06|2009-04-06|\n",
      "|1468023239|Mon Apr 06 23:18:...|   mayveleezet|day 2 is almost o...|day 2 is almost o...|2009|   04| 06|2009-04-06|\n",
      "|1468036593|Mon Apr 06 23:22:...|      johnhays|@realchrissystar ...|@realchrissystar ...|2009|   04| 06|2009-04-06|\n",
      "|1468071517|Mon Apr 06 23:33:...|       JeanneD|Is there Twitter ...|Is there Twitter ...|2009|   04| 06|2009-04-06|\n",
      "|1468087930|Mon Apr 06 23:38:...| _DanielTouch_|is eating frozen ...|is eating frozen ...|2009|   04| 06|2009-04-06|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+----+-----+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, expr\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert the \"Year,\" \"Month,\" and \"Day\" columns to string type\n",
    "df = df.withColumn(\"Year\", df[\"Year\"].cast(StringType()))\n",
    "df = df.withColumn(\"Month\", df[\"Month\"].cast(StringType()))\n",
    "df = df.withColumn(\"Day\", df[\"Day\"].cast(StringType()))\n",
    "\n",
    "# Use `concat_ws` to concatenate the three columns with \"/\" separator\n",
    "df = df.withColumn(\"Date\", concat_ws(\"/\", df[\"Day\"], df[\"Month\"], df[\"Year\"]))\n",
    "\n",
    "# Use `expr` to cast the concatenated string to a date\n",
    "df = df.withColumn(\"Date\", expr(\"to_date(Date, 'dd/MM/yyyy')\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "780bf536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (3.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f35929e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lower, split, explode, regexp_replace\n",
    "\n",
    "# Preprocess the text data\n",
    "df = df.withColumn(\"Tweet_Text\", lower(df[\"Tweet_Text\"]))  # Convert to lowercase\n",
    "df = df.withColumn(\"Tweet_Text\", regexp_replace(df[\"Tweet_Text\"], r\"[^a-z\\s]\", \"\"))  # Remove non-alphabetic characters\n",
    "df = df.withColumn(\"Words\", split(df[\"Tweet_Text\"], \" \"))  # Split text into words\n",
    "\n",
    "# Explode the array of words into separate rows\n",
    "df = df.select(\"Words\").withColumn(\"Word\", explode(df[\"Words\"]))\n",
    "\n",
    "# Group by and count word frequencies\n",
    "word_counts = df.groupBy(\"Word\").count()\n",
    "\n",
    "# Sort words by frequency\n",
    "word_counts = word_counts.orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Convert the result to a Pandas DataFrame for visualization\n",
    "word_counts_pd = word_counts.toPandas()\n",
    "\n",
    "# Display a bar chart of the top N most frequent words\n",
    "top_N = 10  # Set the number of top words to display\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(word_counts_pd[\"Word\"][:top_N], word_counts_pd[\"count\"][:top_N])\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top {} Words in Tweets\".format(top_N))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0687837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, nltk\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/hduser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/home/hduser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed joblib-1.3.2 nltk-3.8.1 regex-2023.10.3 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d1f2ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"Tweet_Text\" among (Words, Word)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9398/4062434765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Assuming you have a DataFrame 'df' with a column 'Tweet_Text'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Preprocess the text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tweet_Text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet_Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tweet_Text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet_Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"[^a-z\\s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove non-alphabetic characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet_Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Split text into words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \"\"\"\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"Tweet_Text\" among (Words, Word)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pyspark.sql.functions import lower, split, explode, regexp_replace, col\n",
    "\n",
    "# Initialize NLTK and download the list of stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a list of NLTK stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with a column 'Tweet_Text'\n",
    "# Preprocess the text data\n",
    "df = df.withColumn(\"Tweet_Text\", lower(df[\"Tweet_Text\"]))  # Convert to lowercase\n",
    "df = df.withColumn(\"Tweet_Text\", regexp_replace(df[\"Tweet_Text\"], r\"[^a-z\\s]\", \"\"))  # Remove non-alphabetic characters\n",
    "df = df.withColumn(\"Words\", split(df[\"Tweet_Text\"], \" \"))  # Split text into words\n",
    "\n",
    "# Explode the array of words into separate rows\n",
    "df = df.select(\"Words\").withColumn(\"Word\", explode(df[\"Words\"]))\n",
    "\n",
    "# Filter out NLTK stopwords\n",
    "df = df.filter(~col(\"Word\").isin(nltk_stopwords))\n",
    "\n",
    "# Group by and count word frequencies\n",
    "word_counts = df.groupBy(\"Word\").count()\n",
    "\n",
    "# Sort words by frequency\n",
    "word_counts = word_counts.orderBy(\"count\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Tokenize the \"Tweet_Text\" column\n",
    "tokenizer = Tokenizer(inputCol=\"Tweet_Text\", outputCol=\"words\")\n",
    "filtered_df = tokenizer.transform(filtered_df)\n",
    "\n",
    "# Remove stopwords from the \"words\" column\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_text\")\n",
    "filtered_df = stopwords_remover.transform(filtered_df)\n",
    "\n",
    "# Show the modified DataFrame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905bc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filtered_df.select(\"filtered_text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912d4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
