{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5aed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69ac448",
   "metadata": {},
   "outputs": [],
   "source": [
    "customSchema = StructType([\n",
    "    StructField(\"Primary_Index\", IntegerType(), True), \n",
    "    StructField(\"Tweet_Id\", StringType(), True), \n",
    "    StructField(\"Date_Text\", StringType(), True),\n",
    "    StructField(\"Flag\", StringType(), True), \n",
    "    StructField(\"User\", StringType(), True),\n",
    "    StructField(\"Tweet_Text\", StringType(), True)])\n",
    "\n",
    "df = spark.read.load('hdfs://localhost:9000/CA2/ProjectTweets.csv', format=\"csv\", header=\"False\", sep=',', schema=customSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f19f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove commas from the Tweet Text field\n",
    "##First test an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7878aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+\n",
      "|Tweet_Text                                                                                         |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?|\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(df[\"Tweet_Id\"] == '1467811594')\n",
    "# Select only the \"Tweet_Text\" column from the filtered DataFrame\n",
    "result = filtered_df.select(\"Tweet_Text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a2a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Then strip the commas out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec30428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+\n",
      "|Tweet_Text                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit only a bit  LOL  I'm fine thanks  how's you ?|\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "# Remove commas from the \"Tweet_Text\" column\n",
    "\n",
    "filtered_df = df.filter(df[\"Tweet_Id\"] == '1467811594')\n",
    "\n",
    "filtered_df = filtered_df.withColumn(\"Tweet_Text\", regexp_replace(filtered_df[\"Tweet_Text\"], \",\", \"\"))\n",
    "\n",
    "# Select only the \"Tweet_Text\" column from the filtered DataFrame\n",
    "result = filtered_df.select(\"Tweet_Text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6edabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now do it with all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c95ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Tweet_Text\", regexp_replace(df[\"Tweet_Text\"], \",\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49093357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+\n",
      "|Tweet_Text                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "|@LOLTrish hey  long time no see! Yes.. Rains a bit only a bit  LOL  I'm fine thanks  how's you ?|\n",
      "+------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(df[\"Tweet_Id\"] == '1467811594')\n",
    "# Select only the \"Tweet_Text\" column from the filtered DataFrame\n",
    "result = filtered_df.select(\"Tweet_Text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "265614ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of Primary_Index: 7\n",
      "Max length of Tweet_Id: 10\n",
      "Max length of Date_Text: 28\n",
      "Max length of Flag: 8\n",
      "Max length of User: 15\n",
      "Max length of Tweet_Text: 374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the max length of each column\n",
    "from pyspark.sql.functions import max, length\n",
    "max_lengths = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    max_length = df.agg(max(length(column_name))).collect()[0][0]\n",
    "    max_lengths.append((column_name, max_length))\n",
    "\n",
    "# Display the results\n",
    "for col, max_len in max_lengths:\n",
    "    print(f\"Max length of {col}: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd852eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in Primary_Index: 0\n",
      "Number of null values in Tweet_Id: 0\n",
      "Number of null values in Date_Text: 0\n",
      "Number of null values in Flag: 0\n",
      "Number of null values in User: 0\n",
      "Number of null values in Tweet_Text: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    count = df.select(column_name).where(col(column_name).isNull()).count()\n",
    "    null_counts.append((column_name, count))\n",
    "\n",
    "# Display the results\n",
    "for col, count in null_counts:\n",
    "    print(f\"Number of null values in {col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f0dc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in Primary_Index: 1600000\n",
      "Number of unique values in Tweet_Id: 1598315\n",
      "Number of unique values in Date_Text: 774363\n",
      "Number of unique values in Flag: 1\n",
      "Number of unique values in User: 659775\n",
      "Number of unique values in Tweet_Text: 1581312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:>                                                         (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Calculate the number of unique values in each column\n",
    "unique_counts = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    count = df.agg(countDistinct(column_name)).collect()[0][0]\n",
    "    unique_counts.append((column_name, count))\n",
    "\n",
    "# Display the results\n",
    "for col, count in unique_counts:\n",
    "    print(f\"Number of unique values in {col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60750a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 78:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  Tweet_Id|count|\n",
      "+----------+-----+\n",
      "|1469531660|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 80:>                                                         (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Find and display an example record with duplicated \"Tweet_Id\"\n",
    "duplicate_tweet_id_example = df.groupBy(\"Tweet_Id\").count().filter(col(\"count\") > 1).limit(1)\n",
    "\n",
    "# Display the result\n",
    "duplicate_tweet_id_example.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5fac47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|Primary_Index|Tweet_Id  |Date_Text                   |Flag    |User        |Tweet_Text                                                       |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|6730         |1469531660|Tue Apr 07 06:39:53 PDT 2009|NO_QUERY|appleaddicto|Company blocked Twitter today  oh well i still have it on mobile |\n",
      "|809639       |1469531660|Tue Apr 07 06:39:53 PDT 2009|NO_QUERY|appleaddicto|Company blocked Twitter today  oh well i still have it on mobile |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find and display all records with the Tweet_Id \"1469531660\"\n",
    "matching_records = df.filter(col(\"Tweet_Id\") == \"1469531660\")\n",
    "\n",
    "# Display the matching records\n",
    "matching_records.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17ea7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with unique records\n",
    "unique_records_df = df.dropDuplicates([\"Tweet_Id\", \"Date_Text\", \"Flag\", \"User\", \"Tweet_Text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a12881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 14:11:52,875 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-28 14:11:53,219 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-28 14:11:53,366 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-28 14:11:53,838 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 120:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in Primary_Index: 1598315\n",
      "Number of unique values in Tweet_Id: 1598315\n",
      "Number of unique values in Date_Text: 774363\n",
      "Number of unique values in Flag: 1\n",
      "Number of unique values in User: 659775\n",
      "Number of unique values in Tweet_Text: 1581312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 122:>                                                        (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique values in each column\n",
    "unique_counts = []\n",
    "\n",
    "for column_name in df.columns:\n",
    "    count = unique_records_df.agg(countDistinct(column_name)).collect()[0][0]\n",
    "    unique_counts.append((column_name, count))\n",
    "\n",
    "# Display the results\n",
    "for col, count in unique_counts:\n",
    "    print(f\"Number of unique values in {col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2921b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 14:14:49,697 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-28 14:14:49,743 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-28 14:14:49,979 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "2023-10-28 14:14:50,055 WARN expressions.RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 130:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|Primary_Index|Tweet_Id  |Date_Text                   |Flag    |User        |Tweet_Text                                                       |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "|6730         |1469531660|Tue Apr 07 06:39:53 PDT 2009|NO_QUERY|appleaddicto|Company blocked Twitter today  oh well i still have it on mobile |\n",
      "+-------------+----------+----------------------------+--------+------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 130:============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Find and display all records with the Tweet_Id \"1469531660\"\n",
    "matching_records = unique_records_df.filter(col(\"Tweet_Id\") == \"1469531660\")\n",
    "\n",
    "# Display the matching records\n",
    "matching_records.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90a0447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unique_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7fce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e3452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ce36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Tokenize the \"Tweet_Text\" column\n",
    "tokenizer = Tokenizer(inputCol=\"Tweet_Text\", outputCol=\"words\")\n",
    "filtered_df = tokenizer.transform(filtered_df)\n",
    "\n",
    "# Remove stopwords from the \"words\" column\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_text\")\n",
    "filtered_df = stopwords_remover.transform(filtered_df)\n",
    "\n",
    "# Show the modified DataFrame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905bc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filtered_df.select(\"filtered_text\")\n",
    "\n",
    "# Show the content of column \"Tweet_Text\"\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912d4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
